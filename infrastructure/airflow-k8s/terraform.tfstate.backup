{
  "version": 4,
  "terraform_version": "1.2.3",
  "serial": 157,
  "lineage": "c8fbfa19-ab23-52d6-d512-49cbf264bf36",
  "outputs": {},
  "resources": [
    {
      "mode": "data",
      "type": "aws_eks_cluster",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:eks:us-east-1:741358071637:cluster/censo-ed-superior-eks",
            "certificate_authority": [
              {
                "data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1Ea3lNVEl5TVRneE5Gb1hEVE15TURreE9ESXlNVGd4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT3ZmClBMWjlENjBaMEhmejVaT3JFSktBekc5Sk82Zk9GcXZPZy9BVnhSR2xVbHc0ZUYxaEgvM25vRDdCVVZqVlllWWQKRlc4V2w5NmRJSEMvcWdreDJ3M2g2QTlhTXdWOFFEUmM1Sm1ULzEzY1NUZmN3T0QzV1VQSnBSOXhuL2M5RXdrNAo5c3dyUVVWdDFWOUd6SFJXaktHNjJxa0h1bUJGSlhhRDVSVDhEc0NjUGlRYUhRQlpIZGNtUE9NZHU1WHV4ajJpCjFUK3hTbThKcFA2QWgrZmpVREJxV2JWM3JHRHJGdldQWTgzenE3RlZuN09YMHIzN0VZSmVVNjE2N0dzOGw1c2YKZ0ZKNnA5WHQ4QTQxRGN2bjZkZmZWMjFlWmJWSUgwZS9hOG5WWEltakQ5ZXR5NkNyMDZXVkk1N21OMXkrQk5BNwpwYVBkbzQxT0tUazUreVBnZE9FQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZEeE11TmpDc2NLYnozaldDT3VaN3AwRlkzaFJNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ2t1TFdETTgyaEJCVWdjcVBBQQowNFdDTzNXb3dickZRbHlUUmdGdlhnV1JLVEdOWEVlUEhDNG9YeHdyOENFbXZVM0ZCT3V2UE55d3VkWk1MUU5WCmtDTFpGZ3hBQ1VuejJUaXFtZXcyWEprc3I5YTA5RWdlUnMraHAybnlveDVNbDByU0tBaVhZbGowSzNWTGVmcTcKd2d2VmttbmV2WmpWRTZyQmlwdVhENHdXZnNrbWNGTWhGcWJyVWR1WGRoT0Y4ZzRiTVljRnlINjJ2eVlNaVIveAp1MkNiL3hxcU04REtRNkxFaTdoUndwTmdtMzNFTFNLQUxsbkhFUUg2QW9rOXRVeEJiYlAxVGpXNkNIMTNZN0RUCkJFeHgzNkdCRW40TVRTSVBOTzNiQUdtQU5MUmZNQXdtdUJmS3NZd1Q1YzVMSVpqZlF0b21vZXI0WkpkOHgyTkYKT2M0PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="
              }
            ],
            "created_at": "2022-09-21 22:12:32.839 +0000 UTC",
            "enabled_cluster_log_types": [
              "api",
              "audit",
              "authenticator"
            ],
            "endpoint": "https://F661015C7BB92FA1F1985413F793B8C0.gr7.us-east-1.eks.amazonaws.com",
            "id": "censo-ed-superior-eks",
            "identity": [
              {
                "oidc": [
                  {
                    "issuer": "https://oidc.eks.us-east-1.amazonaws.com/id/F661015C7BB92FA1F1985413F793B8C0"
                  }
                ]
              }
            ],
            "kubernetes_network_config": [
              {
                "service_ipv4_cidr": "172.20.0.0/16"
              }
            ],
            "name": "censo-ed-superior-eks",
            "platform_version": "eks.2",
            "role_arn": "arn:aws:iam::741358071637:role/censo-ed-superior-eks-cluster-20220921221205925200000001",
            "status": "ACTIVE",
            "tags": {},
            "version": "1.23",
            "vpc_config": [
              {
                "cluster_security_group_id": "sg-0a1afc6f04c505031",
                "endpoint_private_access": false,
                "endpoint_public_access": true,
                "public_access_cidrs": [
                  "0.0.0.0/0"
                ],
                "security_group_ids": [
                  "sg-0859b4dcbef907d7f"
                ],
                "subnet_ids": [
                  "subnet-014b80baf06909a4c",
                  "subnet-05008289a2683b29c",
                  "subnet-0acf9db120d6da8c5"
                ],
                "vpc_id": "vpc-03ed77833152ac9c7"
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_eks_cluster_auth",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "censo-ed-superior-eks",
            "name": "censo-ed-superior-eks",
            "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUEyWkhEQ041SzJUSkk2QkoyJTJGMjAyMjA5MjElMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIyMDkyMVQyMjM3NDZaJlgtQW16LUV4cGlyZXM9MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QlM0J4LWs4cy1hd3MtaWQmWC1BbXotU2lnbmF0dXJlPWE1MDVkODQyNWUwMTNkZGU4ZjYzODQ0MGEwNzZlYjgzOGM5YWM5ODc0MmM3MTQxNTQ1NTc0MjI5Y2ZjNTNiNjM"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "terraform_remote_state",
      "name": "eks",
      "provider": "provider[\"terraform.io/builtin/terraform\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "backend": "s3",
            "config": {
              "value": {
                "bucket": "terraform-state-741358071637",
                "key": "aws-censo-ed-superior/terraform.tfstate",
                "region": "us-east-1"
              },
              "type": [
                "object",
                {
                  "bucket": "string",
                  "key": "string",
                  "region": "string"
                }
              ]
            },
            "defaults": null,
            "outputs": {
              "value": {
                "cluster_endpoint": "https://F661015C7BB92FA1F1985413F793B8C0.gr7.us-east-1.eks.amazonaws.com",
                "cluster_id": "censo-ed-superior-eks",
                "cluster_name": "education-eks-OfGFKC6a",
                "cluster_security_group_id": "sg-0859b4dcbef907d7f",
                "region": "us-east-1"
              },
              "type": [
                "object",
                {
                  "cluster_endpoint": "string",
                  "cluster_id": "string",
                  "cluster_name": "string",
                  "cluster_security_group_id": "string",
                  "region": "string"
                }
              ]
            },
            "workspace": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "airflow",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "airflow",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "airflow",
            "keyring": null,
            "lint": false,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.2.5",
                "chart": "airflow",
                "name": "airflow",
                "namespace": "airflow",
                "revision": 1,
                "values": "{\"affinity\":{},\"airflowConfigAnnotations\":{},\"airflowHome\":\"/opt/airflow\",\"airflowLocalSettings\":\"{{- if semverCompare \\\"\\u003e=2.2.0\\\" .Values.airflowVersion }}\\n{{- if not (or .Values.webserverSecretKey .Values.webserverSecretKeySecretName) }}\\nfrom airflow.www.utils import UIAlert\\n\\nDASHBOARD_UIALERTS = [\\n  UIAlert(\\n    'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'\\n    ' See the \\u003ca href='\\n    '\\\"https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key\\\"\\u003e'\\n    'Helm Chart Production Guide\\u003c/a\\u003e for more details.',\\n    category=\\\"warning\\\",\\n    roles=[\\\"Admin\\\"],\\n    html=True,\\n  )\\n]\\n{{- end }}\\n{{- end }}\",\"airflowPodAnnotations\":{},\"airflowVersion\":\"2.3.0\",\"allowPodLaunching\":true,\"cleanup\":{\"affinity\":{},\"args\":[\"bash\",\"-c\",\"exec airflow kubernetes cleanup-pods --namespace={{ .Release.Namespace }}\"],\"command\":null,\"enabled\":false,\"nodeSelector\":{},\"podAnnotations\":{},\"resources\":{},\"schedule\":\"*/15 * * * *\",\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"tolerations\":[]},\"config\":{\"celery\":{\"worker_concurrency\":16},\"celery_kubernetes_executor\":{\"kubernetes_queue\":\"kubernetes\"},\"core\":{\"colored_console_log\":\"False\",\"dags_folder\":\"{{ include \\\"airflow_dags\\\" . }}\",\"executor\":\"{{ .Values.executor }}\",\"load_examples\":\"False\",\"remote_logging\":\"{{- ternary \\\"True\\\" \\\"False\\\" .Values.elasticsearch.enabled }}\"},\"elasticsearch\":{\"json_format\":\"True\",\"log_id_template\":\"{dag_id}_{task_id}_{execution_date}_{try_number}\"},\"elasticsearch_configs\":{\"max_retries\":3,\"retry_timeout\":\"True\",\"timeout\":30},\"kerberos\":{\"ccache\":\"{{ .Values.kerberos.ccacheMountPath }}/{{ .Values.kerberos.ccacheFileName }}\",\"keytab\":\"{{ .Values.kerberos.keytabPath }}\",\"principal\":\"{{ .Values.kerberos.principal }}\",\"reinit_frequency\":\"{{ .Values.kerberos.reinitFrequency }}\"},\"kubernetes\":{\"airflow_configmap\":\"{{ include \\\"airflow_config\\\" . }}\",\"airflow_local_settings_configmap\":\"{{ include \\\"airflow_config\\\" . }}\",\"multi_namespace_mode\":\"{{ if .Values.multiNamespaceMode }}True{{ else }}False{{ end }}\",\"namespace\":\"{{ .Release.Namespace }}\",\"pod_template_file\":\"{{ include \\\"airflow_pod_template_file\\\" . }}/pod_template_file.yaml\",\"worker_container_repository\":\"{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}\",\"worker_container_tag\":\"{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}\"},\"logging\":{\"colored_console_log\":\"False\",\"remote_logging\":\"{{- ternary \\\"True\\\" \\\"False\\\" .Values.elasticsearch.enabled }}\"},\"metrics\":{\"statsd_host\":\"{{ printf \\\"%s-statsd\\\" .Release.Name }}\",\"statsd_on\":\"{{ ternary \\\"True\\\" \\\"False\\\" .Values.statsd.enabled }}\",\"statsd_port\":9125,\"statsd_prefix\":\"airflow\"},\"scheduler\":{\"run_duration\":41460,\"statsd_host\":\"{{ printf \\\"%s-statsd\\\" .Release.Name }}\",\"statsd_on\":\"{{ ternary \\\"True\\\" \\\"False\\\" .Values.statsd.enabled }}\",\"statsd_port\":9125,\"statsd_prefix\":\"airflow\"},\"webserver\":{\"enable_proxy_fix\":\"True\",\"rbac\":\"True\"}},\"controller\":{\"adminPassword\":\"(sensitive value)\",\"adminUser\":\"(sensitive value)\"},\"createUserJob\":{\"affinity\":{},\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"users create\\\" \\\"create_user\\\" }} \\\"$@\\\"\",\"--\",\"-r\",\"{{ .Values.webserver.defaultUser.role }}\",\"-u\",\"{{ .Values.webserver.defaultUser.username }}\",\"-e\",\"{{ .Values.webserver.defaultUser.email }}\",\"-f\",\"{{ .Values.webserver.defaultUser.firstName }}\",\"-l\",\"{{ .Values.webserver.defaultUser.lastName }}\",\"-p\",\"{{ .Values.webserver.defaultUser.password }}\"],\"command\":null,\"extraContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"jobAnnotations\":{},\"nodeSelector\":{},\"resources\":{},\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"tolerations\":[],\"useHelmHooks\":true},\"dags\":{\"gitSync\":{\"branch\":\"dev\",\"containerName\":\"git-sync\",\"depth\":1,\"enabled\":true,\"env\":[],\"extraVolumeMounts\":[],\"maxFailures\":0,\"repo\":\"https://github.com/MatheusKraisfeld/aws-censo-ed-superior.git\",\"resources\":{},\"rev\":\"HEAD\",\"securityContext\":{},\"subPath\":\"airflow/dags\",\"uid\":65533,\"wait\":60},\"persistence\":{\"accessMode\":\"ReadWriteOnce\",\"enabled\":false,\"existingClaim\":null,\"size\":\"1Gi\",\"storageClassName\":null}},\"data\":{\"brokerUrl\":null,\"brokerUrlSecretName\":null,\"metadataConnection\":{\"db\":\"postgres\",\"host\":null,\"pass\":\"postgres\",\"port\":5432,\"protocol\":\"postgresql\",\"sslmode\":\"disable\",\"user\":\"postgres\"},\"metadataSecretName\":null,\"resultBackendConnection\":null,\"resultBackendSecretName\":null},\"defaultAirflowRepository\":\"apache/airflow\",\"defaultAirflowTag\":\"2.3.0\",\"elasticsearch\":{\"connection\":{},\"enabled\":false,\"secretName\":null},\"enableBuiltInSecretEnvVars\":{\"AIRFLOW_CONN_AIRFLOW_DB\":true,\"AIRFLOW__CELERY__BROKER_URL\":true,\"AIRFLOW__CELERY__CELERY_RESULT_BACKEND\":true,\"AIRFLOW__CELERY__RESULT_BACKEND\":true,\"AIRFLOW__CORE__FERNET_KEY\":true,\"AIRFLOW__CORE__SQL_ALCHEMY_CONN\":true,\"AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST\":true,\"AIRFLOW__ELASTICSEARCH__HOST\":true,\"AIRFLOW__WEBSERVER__SECRET_KEY\":true},\"env\":[{\"name\":\"AIRFLOW__CORE__REMOTE_LOGGING\",\"value\":\"True\"},{\"name\":\"AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER\",\"value\":\"s3://datalake-kraisfeld-igti-edc/airflow-logs/\"},{\"name\":\"AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID\",\"value\":\"my_aws\"}],\"executor\":\"KubernetesExecutor\",\"extraConfigMaps\":{},\"extraEnv\":null,\"extraEnvFrom\":null,\"extraSecrets\":{},\"fernetKey\":\"5q-XGJug2qyBKP-k3KmoMdh349gYb8blAb67XxfiDYg=\",\"fernetKeySecretName\":null,\"flower\":{\"affinity\":{},\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"celery flower\\\" \\\"flower\\\" }}\"],\"command\":null,\"enabled\":true,\"extraContainers\":[],\"extraNetworkPolicies\":[],\"extraVolumes\":[],\"networkPolicy\":{\"ingress\":{\"from\":[],\"ports\":[{\"port\":\"{{ .Values.ports.flowerUI }}\"}]}},\"nodeSelector\":{},\"password\":null,\"podAnnotations\":{},\"priorityClassName\":null,\"resources\":{},\"secretName\":null,\"securityContext\":{},\"service\":{\"annotations\":{},\"loadBalancerIP\":null,\"loadBalancerSourceRanges\":[],\"ports\":[{\"name\":\"flower-ui\",\"port\":\"{{ .Values.ports.flowerUI }}\"}],\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"tolerations\":[],\"username\":null},\"fullnameOverride\":\"\",\"gid\":0,\"images\":{\"airflow\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":null,\"tag\":null},\"flower\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":null,\"tag\":null},\"gitSync\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"k8s.gcr.io/git-sync/git-sync\",\"tag\":\"v3.4.0\"},\"migrationsWaitTimeout\":60,\"pgbouncer\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"apache/airflow\",\"tag\":\"airflow-pgbouncer-2021.04.28-1.14.0\"},\"pgbouncerExporter\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"apache/airflow\",\"tag\":\"airflow-pgbouncer-exporter-2021.09.22-0.12.0\"},\"pod_template\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":null,\"tag\":null},\"redis\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"redis\",\"tag\":\"6-bullseye\"},\"statsd\":{\"pullPolicy\":\"IfNotPresent\",\"repository\":\"apache/airflow\",\"tag\":\"airflow-statsd-exporter-2021.04.28-v0.17.0\"},\"useDefaultImageForMigration\":false},\"ingress\":{\"enabled\":false,\"flower\":{\"annotations\":{},\"host\":\"\",\"hosts\":[],\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"tls\":{\"enabled\":false,\"secretName\":\"\"}},\"web\":{\"annotations\":{},\"host\":\"\",\"hosts\":[],\"ingressClassName\":\"\",\"path\":\"/\",\"pathType\":\"ImplementationSpecific\",\"precedingPaths\":[],\"succeedingPaths\":[],\"tls\":{\"enabled\":false,\"secretName\":\"\"}}},\"kerberos\":{\"ccacheFileName\":\"cache\",\"ccacheMountPath\":\"/var/kerberos-ccache\",\"config\":\"# This is an example config showing how you can use templating and how \\\"example\\\" config\\n# might look like. It works with the test kerberos server that we are using during integration\\n# testing at Apache Airflow (see `scripts/ci/docker-compose/integration-kerberos.yml` but in\\n# order to make it production-ready you must replace it with your own configuration that\\n# Matches your kerberos deployment. Administrators of your Kerberos instance should\\n# provide the right configuration.\\n\\n[logging]\\ndefault = \\\"FILE:{{ template \\\"airflow_logs_no_quote\\\" . }}/kerberos_libs.log\\\"\\nkdc = \\\"FILE:{{ template \\\"airflow_logs_no_quote\\\" . }}/kerberos_kdc.log\\\"\\nadmin_server = \\\"FILE:{{ template \\\"airflow_logs_no_quote\\\" . }}/kadmind.log\\\"\\n\\n[libdefaults]\\ndefault_realm = FOO.COM\\nticket_lifetime = 10h\\nrenew_lifetime = 7d\\nforwardable = true\\n\\n[realms]\\nFOO.COM = {\\n  kdc = kdc-server.foo.com\\n  admin_server = admin_server.foo.com\\n}\\n\",\"configPath\":\"/etc/krb5.conf\",\"enabled\":false,\"keytabBase64Content\":null,\"keytabPath\":\"/etc/airflow.keytab\",\"principal\":\"airflow@FOO.COM\",\"reinitFrequency\":3600},\"kubeVersionOverride\":\"\",\"labels\":{},\"limits\":[],\"logs\":{\"persistence\":{\"enabled\":false,\"existingClaim\":null,\"size\":\"100Gi\",\"storageClassName\":null}},\"migrateDatabaseJob\":{\"affinity\":{},\"annotations\":{},\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"db upgrade\\\" \\\"upgradedb\\\" }}\"],\"command\":null,\"extraContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"jobAnnotations\":{},\"nodeSelector\":{},\"resources\":{},\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"tolerations\":[],\"useHelmHooks\":true},\"multiNamespaceMode\":false,\"nameOverride\":\"\",\"networkPolicies\":{\"enabled\":false},\"nodeSelector\":{},\"pgbouncer\":{\"affinity\":{},\"args\":null,\"ciphers\":\"normal\",\"command\":[\"pgbouncer\",\"-u\",\"nobody\",\"/etc/pgbouncer/pgbouncer.ini\"],\"configSecretName\":null,\"enabled\":false,\"extraIni\":null,\"extraIniMetadata\":null,\"extraIniResultBackend\":null,\"extraNetworkPolicies\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"logConnections\":0,\"logDisconnections\":0,\"maxClientConn\":100,\"metadataPoolSize\":10,\"metricsExporterSidecar\":{\"resources\":{},\"sslmode\":\"disable\"},\"nodeSelector\":{},\"podDisruptionBudget\":{\"config\":{\"maxUnavailable\":1},\"enabled\":false},\"priorityClassName\":null,\"resources\":{},\"resultBackendPoolSize\":5,\"service\":{\"extraAnnotations\":{}},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"ssl\":{\"ca\":null,\"cert\":null,\"key\":null},\"sslmode\":\"prefer\",\"tolerations\":[],\"uid\":65534,\"verbose\":0},\"podTemplate\":null,\"ports\":{\"airflowUI\":8080,\"flowerUI\":5555,\"pgbouncer\":6543,\"pgbouncerScrape\":9127,\"redisDB\":6379,\"statsdIngest\":9125,\"statsdScrape\":9102,\"workerLogs\":8793},\"postgresql\":{\"enabled\":true,\"postgresqlPassword\":\"postgres\",\"postgresqlUsername\":\"postgres\"},\"quotas\":{},\"rbac\":{\"create\":true,\"createSCCRoleBinding\":false},\"redis\":{\"affinity\":{},\"enabled\":false,\"nodeSelector\":{},\"password\":null,\"passwordSecretName\":null,\"persistence\":{\"enabled\":true,\"size\":\"1Gi\",\"storageClassName\":null},\"resources\":{},\"safeToEvict\":true,\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"terminationGracePeriodSeconds\":600,\"tolerations\":[]},\"registry\":{\"connection\":{},\"secretName\":null},\"scheduler\":{\"affinity\":{},\"args\":[\"bash\",\"-c\",\"exec airflow scheduler\"],\"command\":null,\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"livenessProbe\":{\"command\":[\"sh\",\"-c\",\"CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -Wignore -c \\\"\\nimport os\\nos.environ['AIRFLOW__CORE__LOGGING_LEVEL'] = 'ERROR'\\nos.environ['AIRFLOW__LOGGING__LOGGING_LEVEL'] = 'ERROR'\\nfrom airflow.jobs.scheduler_job import SchedulerJob\\nfrom airflow.utils.db import create_session\\nfrom airflow.utils.net import get_hostname\\nimport sys\\nwith create_session() as session:\\n    job = session.query(SchedulerJob).filter_by(hostname=get_hostname()).order_by(\\n        SchedulerJob.latest_heartbeat.desc()).limit(1).first()\\nsys.exit(0 if job.is_alive() else 1)\\\"\\n\"],\"failureThreshold\":5,\"initialDelaySeconds\":10,\"periodSeconds\":60,\"timeoutSeconds\":20},\"logGroomerSidecar\":{\"args\":[\"bash\",\"/clean-logs\"],\"command\":null,\"enabled\":true,\"resources\":{},\"retentionDays\":15},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{\"config\":{\"maxUnavailable\":1},\"enabled\":false},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"safeToEvict\":true,\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"strategy\":null,\"tolerations\":[],\"updateStrategy\":null},\"secret\":[],\"securityContext\":{},\"statsd\":{\"affinity\":{},\"enabled\":true,\"extraMappings\":[],\"extraNetworkPolicies\":[],\"nodeSelector\":{},\"priorityClassName\":null,\"resources\":{},\"securityContext\":{},\"service\":{\"extraAnnotations\":{}},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"tolerations\":[],\"uid\":65534},\"tolerations\":[],\"triggerer\":{\"affinity\":{},\"args\":[\"bash\",\"-c\",\"exec airflow triggerer\"],\"command\":null,\"enabled\":true,\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"livenessProbe\":{\"command\":[\"sh\",\"-c\",\"CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -Wignore -c \\\"\\nimport os\\nos.environ['AIRFLOW__CORE__LOGGING_LEVEL'] = 'ERROR'\\nos.environ['AIRFLOW__LOGGING__LOGGING_LEVEL'] = 'ERROR'\\n\\nfrom airflow.jobs.triggerer_job import TriggererJob\\nfrom airflow.utils.db import create_session\\nfrom airflow.utils.net import get_hostname\\nimport sys\\n\\nwith create_session() as session:\\n    job = session.query(TriggererJob).filter_by(hostname=get_hostname()).order_by(\\n        TriggererJob.latest_heartbeat.desc()).limit(1).first()\\n\\nsys.exit(0 if job.is_alive() else 1)\\n\\\"\\n\"],\"failureThreshold\":5,\"initialDelaySeconds\":10,\"periodSeconds\":60,\"timeoutSeconds\":20},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"safeToEvict\":true,\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"100%\",\"maxUnavailable\":\"50%\"}},\"terminationGracePeriodSeconds\":60,\"tolerations\":[]},\"uid\":50000,\"webserver\":{\"affinity\":{},\"allowPodLogReading\":true,\"args\":[\"bash\",\"-c\",\"exec airflow webserver\"],\"command\":null,\"defaultUser\":{\"email\":\"matheuskraisfeld@gmail.com\",\"enabled\":true,\"firstName\":\"Matheus\",\"lastName\":\"Lima\",\"password\":\"admin\",\"role\":\"Admin\",\"username\":\"matheuskraisfeld\"},\"extraContainers\":[],\"extraInitContainers\":[],\"extraNetworkPolicies\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"livenessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":15,\"periodSeconds\":5,\"timeoutSeconds\":30},\"networkPolicy\":{\"ingress\":{\"from\":[],\"ports\":[{\"port\":\"{{ .Values.ports.airflowUI }}\"}]}},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":null,\"readinessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":15,\"periodSeconds\":5,\"timeoutSeconds\":30},\"replicas\":1,\"resources\":{},\"securityContext\":{},\"service\":{\"annotations\":null,\"loadBalancerIP\":null,\"loadBalancerSourceRanges\":[],\"ports\":[{\"name\":\"airflow-ui\",\"port\":\"{{ .Values.ports.airflowUI }}\"}],\"type\":\"LoadBalancer\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"strategy\":null,\"tolerations\":[],\"webserverConfig\":null},\"webserverSecretKey\":null,\"webserverSecretKeySecretName\":null,\"workers\":{\"affinity\":{},\"args\":[\"bash\",\"-c\",\"exec \\\\\\nairflow {{ semverCompare \\\"\\u003e=2.0.0\\\" .Values.airflowVersion | ternary \\\"celery worker\\\" \\\"worker\\\" }}\"],\"command\":null,\"extraContainers\":[],\"extraInitContainers\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"hostAliases\":[],\"keda\":{\"cooldownPeriod\":30,\"enabled\":false,\"maxReplicaCount\":10,\"minReplicaCount\":0,\"namespaceLabels\":{},\"pollingInterval\":5},\"kerberosSidecar\":{\"enabled\":false,\"resources\":{}},\"logGroomerSidecar\":{\"args\":[\"bash\",\"/clean-logs\"],\"command\":null,\"resources\":{},\"retentionDays\":15},\"nodeSelector\":{},\"persistence\":{\"enabled\":true,\"fixPermissions\":false,\"size\":\"100Gi\",\"storageClassName\":null},\"podAnnotations\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"safeToEvict\":true,\"securityContext\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"strategy\":{\"rollingUpdate\":{\"maxSurge\":\"100%\",\"maxUnavailable\":\"50%\"}},\"terminationGracePeriodSeconds\":600,\"tolerations\":[],\"updateStrategy\":null}}",
                "version": "8.6.1"
              }
            ],
            "name": "airflow",
            "namespace": "airflow",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://airflow-helm.github.io/charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [
              {
                "name": "controller.adminPassword",
                "type": "",
                "value": "admin"
              },
              {
                "name": "controller.adminUser",
                "type": "",
                "value": "admin"
              }
            ],
            "skip_crds": false,
            "status": "failed",
            "timeout": 300,
            "values": [
              "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\n# Default values for airflow.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n# Provide a name to substitute for the full names of resources\nfullnameOverride: \"\"\n\n# Provide a name to substitute for the name of the chart\nnameOverride: \"\"\n\n# Provide a Kubernetes version (used for API Version selection) to override the auto-detected version\nkubeVersionOverride: \"\"\n\n# User and group of airflow user\nuid: 50000\ngid: 0\n\n# Default security context for airflow\nsecurityContext: {}\n#  runAsUser: 50000\n#  fsGroup: 0\n#  runAsGroup: 0\n\n# Airflow home directory\n# Used for mount paths\nairflowHome: /opt/airflow\n\n# Default airflow repository -- overrides all the specific images below\ndefaultAirflowRepository: apache/airflow\n\n# Default airflow tag to deploy\ndefaultAirflowTag: \"2.3.0\"\n\n# Airflow version (Used to make some decisions based on Airflow Version being deployed)\nairflowVersion: \"2.3.0\"\n\n# Images\nimages:\n  airflow:\n    repository: ~\n    tag: ~\n    pullPolicy: IfNotPresent\n  # To avoid images with user code, you can turn this to 'true' and\n  # all the 'run-airflow-migrations' and 'wait-for-airflow-migrations' containers/jobs\n  # will use the images from 'defaultAirflowRepository:defaultAirflowTag' values\n  # to run and wait for DB migrations .\n  useDefaultImageForMigration: false\n  # timeout (in seconds) for airflow-migrations to complete\n  migrationsWaitTimeout: 60\n  pod_template:\n    repository: ~\n    tag: ~\n    pullPolicy: IfNotPresent\n  flower:\n    repository: ~\n    tag: ~\n    pullPolicy: IfNotPresent\n  statsd:\n    repository: apache/airflow\n    tag: airflow-statsd-exporter-2021.04.28-v0.17.0\n    pullPolicy: IfNotPresent\n  redis:\n    repository: redis\n    tag: 6-bullseye\n    pullPolicy: IfNotPresent\n  pgbouncer:\n    repository: apache/airflow\n    tag: airflow-pgbouncer-2021.04.28-1.14.0\n    pullPolicy: IfNotPresent\n  pgbouncerExporter:\n    repository: apache/airflow\n    tag: airflow-pgbouncer-exporter-2021.09.22-0.12.0\n    pullPolicy: IfNotPresent\n  gitSync:\n    repository: k8s.gcr.io/git-sync/git-sync\n    tag: v3.4.0\n    pullPolicy: IfNotPresent\n\n# Select certain nodes for airflow pods.\nnodeSelector: {}\naffinity: {}\ntolerations: []\n\n# Add common labels to all objects and pods defined in this chart.\nlabels: {}\n\n# Ingress configuration\ningress:\n  # Enable ingress resource\n  enabled: false\n\n  # Configs for the Ingress of the web Service\n  web:\n    # Annotations for the web Ingress\n    annotations: {}\n\n    # The path for the web Ingress\n    path: \"/\"\n\n    # The pathType for the above path (used only with Kubernetes v1.19 and above)\n    pathType: \"ImplementationSpecific\"\n\n    # The hostname for the web Ingress (Deprecated - renamed to `ingress.web.hosts`)\n    host: \"\"\n\n    # The hostnames or hosts configuration for the web Ingress\n    hosts: []\n    # - name: \"\"\n    #   # configs for web Ingress TLS\n    #   tls:\n    #     # Enable TLS termination for the web Ingress\n    #     enabled: false\n    #     # the name of a pre-created Secret containing a TLS private key and certificate\n    #     secretName: \"\"\n\n    # The Ingress Class for the web Ingress (used only with Kubernetes v1.19 and above)\n    ingressClassName: \"\"\n\n    # configs for web Ingress TLS (Deprecated - renamed to `ingress.web.hosts[*].tls`)\n    tls:\n      # Enable TLS termination for the web Ingress\n      enabled: false\n      # the name of a pre-created Secret containing a TLS private key and certificate\n      secretName: \"\"\n\n    # HTTP paths to add to the web Ingress before the default path\n    precedingPaths: []\n\n    # Http paths to add to the web Ingress after the default path\n    succeedingPaths: []\n\n  # Configs for the Ingress of the flower Service\n  flower:\n    # Annotations for the flower Ingress\n    annotations: {}\n\n    # The path for the flower Ingress\n    path: \"/\"\n\n    # The pathType for the above path (used only with Kubernetes v1.19 and above)\n    pathType: \"ImplementationSpecific\"\n\n    # The hostname for the flower Ingress (Deprecated - renamed to `ingress.flower.hosts`)\n    host: \"\"\n\n    # The hostnames or hosts configuration for the flower Ingress\n    hosts: []\n    # - name: \"\"\n    #   tls:\n    #     # Enable TLS termination for the flower Ingress\n    #     enabled: false\n    #     # the name of a pre-created Secret containing a TLS private key and certificate\n    #     secretName: \"\"\n\n    # The Ingress Class for the flower Ingress (used only with Kubernetes v1.19 and above)\n    ingressClassName: \"\"\n\n    # configs for flower Ingress TLS (Deprecated - renamed to `ingress.flower.hosts[*].tls`)\n    tls:\n      # Enable TLS termination for the flower Ingress\n      enabled: false\n      # the name of a pre-created Secret containing a TLS private key and certificate\n      secretName: \"\"\n\n# Network policy configuration\nnetworkPolicies:\n  # Enabled network policies\n  enabled: false\n\n# Extra annotations to apply to all\n# Airflow pods\nairflowPodAnnotations: {}\n\n# Extra annotations to apply to\n# main Airflow configmap\nairflowConfigAnnotations: {}\n\n# `airflow_local_settings` file as a string (can be templated).\nairflowLocalSettings: |-\n  {{- if semverCompare \"\u003e=2.2.0\" .Values.airflowVersion }}\n  {{- if not (or .Values.webserverSecretKey .Values.webserverSecretKeySecretName) }}\n  from airflow.www.utils import UIAlert\n\n  DASHBOARD_UIALERTS = [\n    UIAlert(\n      'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'\n      ' See the \u003ca href='\n      '\"https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key\"\u003e'\n      'Helm Chart Production Guide\u003c/a\u003e for more details.',\n      category=\"warning\",\n      roles=[\"Admin\"],\n      html=True,\n    )\n  ]\n  {{- end }}\n  {{- end }}\n\n# Enable RBAC (default on most clusters these days)\nrbac:\n  # Specifies whether RBAC resources should be created\n  create: true\n  createSCCRoleBinding: false\n\n# Airflow executor\n# Options: LocalExecutor, CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor\nexecutor: \"KubernetesExecutor\"\n\n# If this is true and using LocalExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the scheduler's\n# service account will have access to communicate with the api-server and launch pods.\n# If this is true and using CeleryExecutor/KubernetesExecutor/CeleryKubernetesExecutor, the workers\n# will be able to launch pods.\nallowPodLaunching: true\n\n# Environment variables for all airflow containers\nenv: \n  - name: AIRFLOW__CORE__REMOTE_LOGGING\n    value: 'True'\n  - name: AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER\n    value: \"s3://datalake-kraisfeld-igti-edc/airflow-logs/\"\n  - name: AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID\n    value: \"my_aws\"\n# - name: \"\"\n#   value: \"\"\n\n# Secrets for all airflow containers\nsecret: []\n# - envName: \"\"\n#   secretName: \"\"\n#   secretKey: \"\"\n\n# Enables selected built-in secrets that are set via environment variables by default.\n# Those secrets are provided by the Helm Chart secrets by default but in some cases you\n# might want to provide some of those variables with _CMD or _SECRET variable, and you should\n# in this case disable setting of those variables by setting the relevant configuration to false.\nenableBuiltInSecretEnvVars:\n  AIRFLOW__CORE__FERNET_KEY: true\n  AIRFLOW__CORE__SQL_ALCHEMY_CONN: true\n  AIRFLOW_CONN_AIRFLOW_DB: true\n  AIRFLOW__WEBSERVER__SECRET_KEY: true\n  AIRFLOW__CELERY__CELERY_RESULT_BACKEND: true\n  AIRFLOW__CELERY__RESULT_BACKEND: true\n  AIRFLOW__CELERY__BROKER_URL: true\n  AIRFLOW__ELASTICSEARCH__HOST: true\n  AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST: true\n\n# Extra secrets that will be managed by the chart\n# (You can use them with extraEnv or extraEnvFrom or some of the extraVolumes values).\n# The format is \"key/value\" where\n#    * key (can be templated) is the name of the secret that will be created\n#    * value: an object with the standard 'data' or 'stringData' key (or both).\n#          The value associated with those keys must be a string (can be templated)\nextraSecrets: {}\n# eg:\n# extraSecrets:\n#   '{{ .Release.Name }}-airflow-connections':\n#     type: 'Opaque'\n#     data: |\n#       AIRFLOW_CONN_GCP: 'base64_encoded_gcp_conn_string'\n#       AIRFLOW_CONN_AWS: 'base64_encoded_aws_conn_string'\n#     stringData: |\n#       AIRFLOW_CONN_OTHER: 'other_conn'\n#   '{{ .Release.Name }}-other-secret-name-suffix':\n#     data: |\n#        ...\n\n# Extra ConfigMaps that will be managed by the chart\n# (You can use them with extraEnv or extraEnvFrom or some of the extraVolumes values).\n# The format is \"key/value\" where\n#    * key (can be templated) is the name of the configmap that will be created\n#    * value: an object with the standard 'data' key.\n#          The value associated with this keys must be a string (can be templated)\nextraConfigMaps: {}\n# eg:\n# extraConfigMaps:\n#   '{{ .Release.Name }}-airflow-variables':\n#     data: |\n#       AIRFLOW_VAR_HELLO_MESSAGE: \"Hi!\"\n#       AIRFLOW_VAR_KUBERNETES_NAMESPACE: \"{{ .Release.Namespace }}\"\n\n# Extra env 'items' that will be added to the definition of airflow containers\n# a string is expected (can be templated).\n# TODO: difference from `env`? This is a templated string. Probably should template `env` and remove this.\nextraEnv: ~\n# eg:\n# extraEnv: |\n#   - name: AIRFLOW__CORE__LOAD_EXAMPLES\n#     value: 'True'\n\n# Extra envFrom 'items' that will be added to the definition of airflow containers\n# A string is expected (can be templated).\nextraEnvFrom: ~\n# eg:\n# extraEnvFrom: |\n#   - secretRef:\n#       name: '{{ .Release.Name }}-airflow-connections'\n#   - configMapRef:\n#       name: '{{ .Release.Name }}-airflow-variables'\n\n# Airflow database \u0026 redis config\ndata:\n  # If secret names are provided, use those secrets\n  metadataSecretName: ~\n  resultBackendSecretName: ~\n  brokerUrlSecretName: ~\n\n  # Otherwise pass connection values in\n  metadataConnection:\n    user: postgres\n    pass: postgres\n    protocol: postgresql\n    host: ~\n    port: 5432\n    db: postgres\n    sslmode: disable\n  # resultBackendConnection defaults to the same database as metadataConnection\n  resultBackendConnection: ~\n  # or, you can use a different database\n  # resultBackendConnection:\n  #   user: postgres\n  #   pass: postgres\n  #   protocol: postgresql\n  #   host: ~\n  #   port: 5432\n  #   db: postgres\n  #   sslmode: disable\n  # Note: brokerUrl can only be set during install, not upgrade\n  brokerUrl: ~\n\n# Fernet key settings\n# Note: fernetKey can only be set during install, not upgrade\nfernetKey: \"5q-XGJug2qyBKP-k3KmoMdh349gYb8blAb67XxfiDYg=\"\nfernetKeySecretName: ~\n\n# Flask secret key for Airflow Webserver: `[webserver] secret_key` in airflow.cfg\nwebserverSecretKey: ~\nwebserverSecretKeySecretName: ~\n\n# In order to use kerberos you need to create secret containing the keytab file\n# The secret name should follow naming convention of the application where resources are\n# name {{ .Release-name }}-\u003cPOSTFIX\u003e. In case of the keytab file, the postfix is \"kerberos-keytab\"\n# So if your release is named \"my-release\" the name of the secret should be \"my-release-kerberos-keytab\"\n#\n# The Keytab content should be available in the \"kerberos.keytab\" key of the secret.\n#\n#  apiVersion: v1\n#  kind: Secret\n#  data:\n#    kerberos.keytab: \u003cbase64_encoded keytab file content\u003e\n#  type: Opaque\n#\n#\n#  If you have such keytab file you can do it with similar\n#\n#  kubectl create secret generic {{ .Release.name }}-kerberos-keytab --from-file=kerberos.keytab\n#\n#\n#  Alternatively, instead of manually creating the secret, it is possible to specify\n#  kerberos.keytabBase64Content parameter. This parameter should contain base64 encoded keytab.\n#\n\nkerberos:\n  enabled: false\n  ccacheMountPath: /var/kerberos-ccache\n  ccacheFileName: cache\n  configPath: /etc/krb5.conf\n  keytabBase64Content: ~\n  keytabPath: /etc/airflow.keytab\n  principal: airflow@FOO.COM\n  reinitFrequency: 3600\n  config: |\n    # This is an example config showing how you can use templating and how \"example\" config\n    # might look like. It works with the test kerberos server that we are using during integration\n    # testing at Apache Airflow (see `scripts/ci/docker-compose/integration-kerberos.yml` but in\n    # order to make it production-ready you must replace it with your own configuration that\n    # Matches your kerberos deployment. Administrators of your Kerberos instance should\n    # provide the right configuration.\n\n    [logging]\n    default = \"FILE:{{ template \"airflow_logs_no_quote\" . }}/kerberos_libs.log\"\n    kdc = \"FILE:{{ template \"airflow_logs_no_quote\" . }}/kerberos_kdc.log\"\n    admin_server = \"FILE:{{ template \"airflow_logs_no_quote\" . }}/kadmind.log\"\n\n    [libdefaults]\n    default_realm = FOO.COM\n    ticket_lifetime = 10h\n    renew_lifetime = 7d\n    forwardable = true\n\n    [realms]\n    FOO.COM = {\n      kdc = kdc-server.foo.com\n      admin_server = admin_server.foo.com\n    }\n\n# Airflow Worker Config\nworkers:\n  # Number of airflow celery workers in StatefulSet\n  replicas: 1\n\n  # Command to use when running Airflow workers (templated).\n  command: ~\n  # Args to use when running Airflow workers (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    # The format below is necessary to get `helm lint` happy\n    - |-\n      exec \\\n      airflow {{ semverCompare \"\u003e=2.0.0\" .Values.airflowVersion | ternary \"celery worker\" \"worker\" }}\n\n  # Update Strategy when worker is deployed as a StatefulSet\n  updateStrategy: ~\n  # Update Strategy when worker is deployed as a Deployment\n  strategy:\n    rollingUpdate:\n      maxSurge: \"100%\"\n      maxUnavailable: \"50%\"\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  # Allow KEDA autoscaling.\n  # Persistence.enabled must be set to false to use KEDA.\n  keda:\n    enabled: false\n    namespaceLabels: {}\n\n    # How often KEDA polls the airflow DB to report new scale requests to the HPA\n    pollingInterval: 5\n\n    # How many seconds KEDA will wait before scaling to zero.\n    # Note that HPA has a separate cooldown period for scale-downs\n    cooldownPeriod: 30\n\n    # Minimum number of workers created by keda\n    minReplicaCount: 0\n\n    # Maximum number of workers created by keda\n    maxReplicaCount: 10\n\n  persistence:\n    # Enable persistent volumes\n    enabled: true\n    # Volume size for worker StatefulSet\n    size: 100Gi\n    # If using a custom storageClass, pass name ref to all statefulSets here\n    storageClassName:\n    # Execute init container to chown log directory.\n    # This is currently only needed in kind, due to usage\n    # of local-path provisioner.\n    fixPermissions: false\n\n  kerberosSidecar:\n    # Enable kerberos sidecar\n    enabled: false\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Grace period for tasks to finish after SIGTERM is sent from kubernetes\n  terminationGracePeriodSeconds: 600\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Launch additional containers into worker.\n  # Note: If used with KubernetesExecutor, you are responsible for signaling sidecars to exit when the main\n  # container finishes so Airflow can continue the worker shutdown process!\n  extraContainers: []\n  # Add additional init containers into workers.\n  extraInitContainers: []\n\n  # Mount additional volumes into worker.\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow worker pods.\n  nodeSelector: {}\n  priorityClassName: ~\n  affinity: {}\n  # default worker affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: worker\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n  # hostAliases to use in worker pods.\n  # See:\n  # https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  hostAliases: []\n  # - ip: \"127.0.0.2\"\n  #   hostnames:\n  #   - \"test.hostname.one\"\n  # - ip: \"127.0.0.3\"\n  #   hostnames:\n  #   - \"test.hostname.two\"\n\n  podAnnotations: {}\n\n  logGroomerSidecar:\n    # Command to use when running the Airflow worker log groomer sidecar (templated).\n    command: ~\n    # Args to use when running the Airflow worker log groomer sidecar (templated).\n    args: [\"bash\", \"/clean-logs\"]\n    # Number of days to retain logs\n    retentionDays: 15\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n# Airflow scheduler settings\nscheduler:\n  # If the scheduler stops heartbeating for 5 minutes (5*60s) kill the\n  # scheduler and let Kubernetes restart it\n  livenessProbe:\n    initialDelaySeconds: 10\n    timeoutSeconds: 20\n    failureThreshold: 5\n    periodSeconds: 60\n    command:\n      - sh\n      - -c\n      - |\n        CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -Wignore -c \"\n        import os\n        os.environ['AIRFLOW__CORE__LOGGING_LEVEL'] = 'ERROR'\n        os.environ['AIRFLOW__LOGGING__LOGGING_LEVEL'] = 'ERROR'\n        from airflow.jobs.scheduler_job import SchedulerJob\n        from airflow.utils.db import create_session\n        from airflow.utils.net import get_hostname\n        import sys\n        with create_session() as session:\n            job = session.query(SchedulerJob).filter_by(hostname=get_hostname()).order_by(\n                SchedulerJob.latest_heartbeat.desc()).limit(1).first()\n        sys.exit(0 if job.is_alive() else 1)\"\n\n  # Airflow 2.0 allows users to run multiple schedulers,\n  # However this feature is only recommended for MySQL 8+ and Postgres\n  replicas: 1\n\n  # Command to use when running the Airflow scheduler (templated).\n  command: ~\n  # Args to use when running the Airflow scheduler (templated).\n  args: [\"bash\", \"-c\", \"exec airflow scheduler\"]\n\n  # Update Strategy when scheduler is deployed as a StatefulSet\n  # (when using LocalExecutor and workers.persistence)\n  updateStrategy: ~\n  # Update Strategy when scheduler is deployed as a Deployment\n  # (when not using LocalExecutor and workers.persistence)\n  strategy: ~\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to scheduler kubernetes service account.\n    annotations: {}\n\n  # Scheduler pod disruption budget\n  podDisruptionBudget:\n    enabled: false\n\n    # PDB configuration\n    config:\n      maxUnavailable: 1\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Launch additional containers into scheduler.\n  extraContainers: []\n  # Add additional init containers into scheduler.\n  extraInitContainers: []\n\n  # Mount additional volumes into scheduler.\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow scheduler pods.\n  nodeSelector: {}\n  affinity: {}\n  # default scheduler affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: scheduler\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n\n  priorityClassName: ~\n\n  podAnnotations: {}\n\n  logGroomerSidecar:\n    # Whether to deploy the Airflow scheduler log groomer sidecar.\n    enabled: true\n    # Command to use when running the Airflow scheduler log groomer sidecar (templated).\n    command: ~\n    # Args to use when running the Airflow scheduler log groomer sidecar (templated).\n    args: [\"bash\", \"/clean-logs\"]\n    # Number of days to retain logs\n    retentionDays: 15\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\n# Airflow create user job settings\ncreateUserJob:\n  # Command to use when running the create user job (templated).\n  command: ~\n  # Args to use when running the create user job (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    # The format below is necessary to get `helm lint` happy\n    - |-\n      exec \\\n      airflow {{ semverCompare \"\u003e=2.0.0\" .Values.airflowVersion | ternary \"users create\" \"create_user\" }} \"$@\"\n    - --\n    - \"-r\"\n    - \"{{ .Values.webserver.defaultUser.role }}\"\n    - \"-u\"\n    - \"{{ .Values.webserver.defaultUser.username }}\"\n    - \"-e\"\n    - \"{{ .Values.webserver.defaultUser.email }}\"\n    - \"-f\"\n    - \"{{ .Values.webserver.defaultUser.firstName }}\"\n    - \"-l\"\n    - \"{{ .Values.webserver.defaultUser.lastName }}\"\n    - \"-p\"\n    - \"{{ .Values.webserver.defaultUser.password }}\"\n\n  # Annotations on the create user job pod\n  annotations: {}\n  # jobAnnotations are annotations on the create user job\n  jobAnnotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to create user kubernetes service account.\n    annotations: {}\n\n  # Launch additional containers into user creation job\n  extraContainers: []\n\n  # Mount additional volumes into user creation job\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  # In case you need to disable the helm hooks that create the jobs after install.\n  # Disable this if you are using ArgoCD for example\n  useHelmHooks: true\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n# Airflow database migration job settings\nmigrateDatabaseJob:\n  # Command to use when running the migrate database job (templated).\n  command: ~\n  # Args to use when running the migrate database job (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    # The format below is necessary to get `helm lint` happy\n    - |-\n      exec \\\n      airflow {{ semverCompare \"\u003e=2.0.0\" .Values.airflowVersion | ternary \"db upgrade\" \"upgradedb\" }}\n\n  # Annotations on the database migration pod\n  annotations: {}\n  # jobAnnotations are annotations on the database migration job\n  jobAnnotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to migrate database job kubernetes service account.\n    annotations: {}\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Launch additional containers into database migration job\n  extraContainers: []\n\n  # Mount additional volumes into database migration job\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n  # In case you need to disable the helm hooks that create the jobs after install.\n  # Disable this if you are using ArgoCD for example\n  useHelmHooks: true\n\n# Airflow webserver settings\nwebserver:\n  allowPodLogReading: true\n  livenessProbe:\n    initialDelaySeconds: 15\n    timeoutSeconds: 30\n    failureThreshold: 20\n    periodSeconds: 5\n\n  readinessProbe:\n    initialDelaySeconds: 15\n    timeoutSeconds: 30\n    failureThreshold: 20\n    periodSeconds: 5\n\n  # Number of webservers\n  replicas: 1\n\n  # Command to use when running the Airflow webserver (templated).\n  command: ~\n  # Args to use when running the Airflow webserver (templated).\n  args: [\"bash\", \"-c\", \"exec airflow webserver\"]\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to webserver kubernetes service account.\n    annotations: {}\n\n  # Allow overriding Update Strategy for Webserver\n  strategy: ~\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Additional network policies as needed (Deprecated - renamed to `webserver.networkPolicy.ingress.from`)\n  extraNetworkPolicies: []\n  networkPolicy:\n    ingress:\n      # Peers for webserver NetworkPolicy ingress\n      from: []\n      # Ports for webserver NetworkPolicy ingress (if `from` is set)\n      ports:\n        - port: \"{{ .Values.ports.airflowUI }}\"\n\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n\n  # Create initial user.\n  defaultUser:\n    enabled: true\n    role: Admin\n    username: matheuskraisfeld\n    email: matheuskraisfeld@gmail.com\n    firstName: Matheus\n    lastName: Lima\n    password: admin\n\n  # Launch additional containers into webserver.\n  extraContainers: []\n  # Add additional init containers into webserver.\n  extraInitContainers: []\n\n  # Mount additional volumes into webserver.\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # This string (can be templated) will be mounted into the Airflow Webserver as a custom\n  # webserver_config.py. You can bake a webserver_config.py in to your image instead.\n  webserverConfig: ~\n  # webserverConfig: |\n  #   from airflow import configuration as conf\n\n  #   # The SQLAlchemy connection string.\n  #   SQLALCHEMY_DATABASE_URI = conf.get('core', 'SQL_ALCHEMY_CONN')\n\n  #   # Flask-WTF flag for CSRF\n  #   CSRF_ENABLED = True\n\n  service:\n    type: LoadBalancer\n    ## service annotations\n    annotations: \n      \n    ports:\n      - name: airflow-ui\n        port: \"{{ .Values.ports.airflowUI }}\"\n    # To change the port used to access the webserver:\n    # ports:\n    #   - name: airflow-ui\n    #     port: 80\n    #     targetPort: airflow-ui\n    # To only expose a sidecar, not the webserver directly:\n    # ports:\n    #   - name: only_sidecar\n    #     port: 80\n    #     targetPort: 8888\n    loadBalancerIP: ~\n    ## Limit load balancer source ips to list of CIDRs\n    # loadBalancerSourceRanges:\n    #   - \"10.123.0.0/16\"\n    loadBalancerSourceRanges: []\n\n  # Select certain nodes for airflow webserver pods.\n  nodeSelector: {}\n  priorityClassName: ~\n  affinity: {}\n  # default webserver affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: webserver\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n\n  podAnnotations: {}\n\n# Airflow Triggerer Config\ntriggerer:\n  enabled: true\n  # Number of airflow triggerers in the deployment\n  replicas: 1\n\n  # Command to use when running Airflow triggerers (templated).\n  command: ~\n  # Args to use when running Airflow triggerer (templated).\n  args: [\"bash\", \"-c\", \"exec airflow triggerer\"]\n\n  # Update Strategy for triggerers\n  strategy:\n    rollingUpdate:\n      maxSurge: \"100%\"\n      maxUnavailable: \"50%\"\n\n  # If the triggerer stops heartbeating for 5 minutes (5*60s) kill the\n  # triggerer and let Kubernetes restart it\n  livenessProbe:\n    initialDelaySeconds: 10\n    timeoutSeconds: 20\n    failureThreshold: 5\n    periodSeconds: 60\n    command:\n      - sh\n      - -c\n      - |\n        CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -Wignore -c \"\n        import os\n        os.environ['AIRFLOW__CORE__LOGGING_LEVEL'] = 'ERROR'\n        os.environ['AIRFLOW__LOGGING__LOGGING_LEVEL'] = 'ERROR'\n\n        from airflow.jobs.triggerer_job import TriggererJob\n        from airflow.utils.db import create_session\n        from airflow.utils.net import get_hostname\n        import sys\n\n        with create_session() as session:\n            job = session.query(TriggererJob).filter_by(hostname=get_hostname()).order_by(\n                TriggererJob.latest_heartbeat.desc()).limit(1).first()\n\n        sys.exit(0 if job.is_alive() else 1)\n        \"\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to triggerer kubernetes service account.\n    annotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Grace period for triggerer to finish after SIGTERM is sent from kubernetes\n  terminationGracePeriodSeconds: 60\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Launch additional containers into triggerer.\n  extraContainers: []\n  # Add additional init containers into triggerers.\n  extraInitContainers: []\n\n  # Mount additional volumes into triggerer.\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for airflow triggerer pods.\n  nodeSelector: {}\n  affinity: {}\n  # default triggerer affinity is:\n  #  podAntiAffinity:\n  #    preferredDuringSchedulingIgnoredDuringExecution:\n  #    - podAffinityTerm:\n  #        labelSelector:\n  #          matchLabels:\n  #            component: triggerer\n  #        topologyKey: kubernetes.io/hostname\n  #      weight: 100\n  tolerations: []\n\n  priorityClassName: ~\n\n  podAnnotations: {}\n\n# Flower settings\nflower:\n  # Enable flower.\n  # If True, and using CeleryExecutor/CeleryKubernetesExecutor, will deploy flower app.\n  enabled: true\n\n  # Command to use when running flower (templated).\n  command: ~\n  # Args to use when running flower (templated).\n  args:\n    - \"bash\"\n    - \"-c\"\n    # The format below is necessary to get `helm lint` happy\n    - |-\n      exec \\\n      airflow {{ semverCompare \"\u003e=2.0.0\" .Values.airflowVersion | ternary \"celery flower\" \"flower\" }}\n\n  # Additional network policies as needed (Deprecated - renamed to `flower.networkPolicy.ingress.from`)\n  extraNetworkPolicies: []\n  networkPolicy:\n    ingress:\n      # Peers for flower NetworkPolicy ingress\n      from: []\n      # Ports for flower NetworkPolicy ingress (if ingressPeers is set)\n      ports:\n        - port: \"{{ .Values.ports.flowerUI }}\"\n\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  # A secret containing the connection\n  secretName: ~\n\n  # Else, if username and password are set, create secret from username and password\n  username: ~\n  password: ~\n\n  service:\n    type: ClusterIP\n    ## service annotations\n    annotations: {}\n    ports:\n      - name: flower-ui\n        port: \"{{ .Values.ports.flowerUI }}\"\n    # To change the port used to access flower:\n    # ports:\n    #   - name: flower-ui\n    #     port: 8080\n    #     targetPort: flower-ui\n    loadBalancerIP: ~\n    ## Limit load balancer source ips to list of CIDRs\n    # loadBalancerSourceRanges:\n    #   - \"10.123.0.0/16\"\n    loadBalancerSourceRanges: []\n\n  # Launch additional containers into the flower pods.\n  extraContainers: []\n  # Mount additional volumes into the flower pods.\n  extraVolumes: []\n\n  # Select certain nodes for airflow flower pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n\n  priorityClassName: ~\n\n  podAnnotations: {}\n\n# Statsd settings\nstatsd:\n  enabled: true\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 65534\n  #  fsGroup: 0\n  #  runAsGroup: 0\n\n  # Additional network policies as needed\n  extraNetworkPolicies: []\n  resources: {}\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n\n  service:\n    extraAnnotations: {}\n\n  # Select certain nodes for statsd pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n\n  priorityClassName: ~\n\n  # Additional mappings for statsd exporter.\n  extraMappings: []\n\n  uid: 65534\n\n# PgBouncer settings\npgbouncer:\n  # Enable PgBouncer\n  enabled: false\n  # Command to use for PgBouncer(templated).\n  command: [\"pgbouncer\", \"-u\", \"nobody\", \"/etc/pgbouncer/pgbouncer.ini\"]\n  # Args to use for PgBouncer(templated).\n  args: ~\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  # Additional network policies as needed\n  extraNetworkPolicies: []\n\n  # Pool sizes\n  metadataPoolSize: 10\n  resultBackendPoolSize: 5\n\n  # Maximum clients that can connect to PgBouncer (higher = more file descriptors)\n  maxClientConn: 100\n\n  # supply the name of existing secret with pgbouncer.ini and users.txt defined\n  # you can load them to a k8s secret like the one below\n  #  apiVersion: v1\n  #  kind: Secret\n  #  metadata:\n  #    name: pgbouncer-config-secret\n  #  data:\n  #     pgbouncer.ini: \u003cbase64_encoded pgbouncer.ini file content\u003e\n  #     users.txt: \u003cbase64_encoded users.txt file content\u003e\n  #  type: Opaque\n  #\n  #  configSecretName: pgbouncer-config-secret\n  #\n  configSecretName: ~\n\n  # PgBouncer pod disruption budget\n  podDisruptionBudget:\n    enabled: false\n\n    # PDB configuration\n    config:\n      maxUnavailable: 1\n\n  # Limit the resources to PgBouncer.\n  # When you specify the resource request the k8s scheduler uses this information to decide which node to\n  # place the Pod on. When you specify a resource limit for a Container, the kubelet enforces those limits so\n  # that the running container is not allowed to use more of that resource than the limit you set.\n  # See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n  # Example:\n  #\n  # resource:\n  #   limits:\n  #     cpu: 100m\n  #     memory: 128Mi\n  #   requests:\n  #     cpu: 100m\n  #     memory: 128Mi\n  resources: {}\n\n  service:\n    extraAnnotations: {}\n\n  # https://www.pgbouncer.org/config.html\n  verbose: 0\n  logDisconnections: 0\n  logConnections: 0\n\n  sslmode: \"prefer\"\n  ciphers: \"normal\"\n\n  ssl:\n    ca: ~\n    cert: ~\n    key: ~\n\n  # Add extra PgBouncer ini configuration in the databases section:\n  # https://www.pgbouncer.org/config.html#section-databases\n  extraIniMetadata: ~\n  extraIniResultBackend: ~\n  # Add extra general PgBouncer ini configuration: https://www.pgbouncer.org/config.html\n  extraIni: ~\n\n  # Mount additional volumes into pgbouncer.\n  extraVolumes: []\n  extraVolumeMounts: []\n\n  # Select certain nodes for PgBouncer pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n\n  priorityClassName: ~\n\n  uid: 65534\n\n  metricsExporterSidecar:\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n    sslmode: \"disable\"\n\n# Configuration for the redis provisioned by the chart\nredis:\n  enabled: false\n  terminationGracePeriodSeconds: 600\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to worker kubernetes service account.\n    annotations: {}\n\n  persistence:\n    # Enable persistent volumes\n    enabled: true\n    # Volume size for worker StatefulSet\n    size: 1Gi\n    # If using a custom storageClass, pass name ref to all statefulSets here\n    storageClassName:\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # If set use as redis secret. Make sure to also set data.brokerUrlSecretName value.\n  passwordSecretName: ~\n\n  # Else, if password is set, create secret with it,\n  # Otherwise a new password will be generated on install\n  # Note: password can only be set during install, not upgrade.\n  password: ~\n\n  # This setting tells kubernetes that its ok to evict\n  # when it wants to scale a node down.\n  safeToEvict: true\n\n  # Select certain nodes for redis pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n\n# Auth secret for a private registry\n# This is used if pulling airflow images from a private registry\nregistry:\n  secretName: ~\n\n  # Example:\n  # connection:\n  #   user: ~\n  #   pass: ~\n  #   host: ~\n  #   email: ~\n  connection: {}\n\n# Elasticsearch logging configuration\nelasticsearch:\n  # Enable elasticsearch task logging\n  enabled: false\n  # A secret containing the connection\n  secretName: ~\n  # Or an object representing the connection\n  # Example:\n  # connection:\n  #   user: ~\n  #   pass: ~\n  #   host: ~\n  #   port: ~\n  connection: {}\n\n# All ports used by chart\nports:\n  flowerUI: 5555\n  airflowUI: 8080\n  workerLogs: 8793\n  redisDB: 6379\n  statsdIngest: 9125\n  statsdScrape: 9102\n  pgbouncer: 6543\n  pgbouncerScrape: 9127\n\n# Define any ResourceQuotas for namespace\nquotas: {}\n\n# Define default/max/min values for pods and containers in namespace\nlimits: []\n\n# This runs as a CronJob to cleanup old pods.\ncleanup:\n  enabled: false\n  # Run every 15 minutes\n  schedule: \"*/15 * * * *\"\n  # Command to use when running the cleanup cronjob (templated).\n  command: ~\n  # Args to use when running the cleanup cronjob (templated).\n  args: [\"bash\", \"-c\", \"exec airflow kubernetes cleanup-pods --namespace={{ .Release.Namespace }}\"]\n\n\n  # Select certain nodes for airflow cleanup pods.\n  nodeSelector: {}\n  affinity: {}\n  tolerations: []\n\n  podAnnotations: {}\n\n  resources: {}\n  #  limits:\n  #   cpu: 100m\n  #   memory: 128Mi\n  #  requests:\n  #   cpu: 100m\n  #   memory: 128Mi\n\n  # Create ServiceAccount\n  serviceAccount:\n    # Specifies whether a ServiceAccount should be created\n    create: true\n    # The name of the ServiceAccount to use.\n    # If not set and create is true, a name is generated using the release name\n    name: ~\n\n    # Annotations to add to cleanup cronjob kubernetes service account.\n    annotations: {}\n\n  # When not set, the values defined in the global securityContext will be used\n  securityContext: {}\n  #  runAsUser: 50000\n  #  runAsGroup: 0\n\n# Configuration for postgresql subchart\n# Not recommended for production\npostgresql:\n  enabled: true\n  postgresqlPassword: postgres\n  postgresqlUsername: postgres\n\n# Config settings to go into the mounted airflow.cfg\n#\n# Please note that these values are passed through the `tpl` function, so are\n# all subject to being rendered as go templates. If you need to include a\n# literal `{{` in a value, it must be expressed like this:\n#\n#    a: '{{ \"{{ not a template }}\" }}'\n#\n# Do not set config containing secrets via plain text values, use Env Var or k8s secret object\n# yamllint disable rule:line-length\nconfig:\n  core:\n    dags_folder: '{{ include \"airflow_dags\" . }}'\n    # This is ignored when used with the official Docker image\n    load_examples: 'False'\n    executor: '{{ .Values.executor }}'\n    # For Airflow 1.10, backward compatibility; moved to [logging] in 2.0\n    colored_console_log: 'False'\n    remote_logging: '{{- ternary \"True\" \"False\" .Values.elasticsearch.enabled }}'\n  logging:\n    remote_logging: '{{- ternary \"True\" \"False\" .Values.elasticsearch.enabled }}'\n    colored_console_log: 'False'\n  metrics:\n    statsd_on: '{{ ternary \"True\" \"False\" .Values.statsd.enabled }}'\n    statsd_port: 9125\n    statsd_prefix: airflow\n    statsd_host: '{{ printf \"%s-statsd\" .Release.Name }}'\n  webserver:\n    enable_proxy_fix: 'True'\n    # For Airflow 1.10\n    rbac: 'True'\n  celery:\n    worker_concurrency: 16\n  scheduler:\n    # statsd params included for Airflow 1.10 backward compatibility; moved to [metrics] in 2.0\n    statsd_on: '{{ ternary \"True\" \"False\" .Values.statsd.enabled }}'\n    statsd_port: 9125\n    statsd_prefix: airflow\n    statsd_host: '{{ printf \"%s-statsd\" .Release.Name }}'\n    # `run_duration` included for Airflow 1.10 backward compatibility; removed in 2.0.\n    run_duration: 41460\n  elasticsearch:\n    json_format: 'True'\n    log_id_template: \"{dag_id}_{task_id}_{execution_date}_{try_number}\"\n  elasticsearch_configs:\n    max_retries: 3\n    timeout: 30\n    retry_timeout: 'True'\n  kerberos:\n    keytab: '{{ .Values.kerberos.keytabPath }}'\n    reinit_frequency: '{{ .Values.kerberos.reinitFrequency }}'\n    principal: '{{ .Values.kerberos.principal }}'\n    ccache: '{{ .Values.kerberos.ccacheMountPath }}/{{ .Values.kerberos.ccacheFileName }}'\n  celery_kubernetes_executor:\n    kubernetes_queue: 'kubernetes'\n  kubernetes:\n    namespace: '{{ .Release.Namespace }}'\n    airflow_configmap: '{{ include \"airflow_config\" . }}'\n    airflow_local_settings_configmap: '{{ include \"airflow_config\" . }}'\n    pod_template_file: '{{ include \"airflow_pod_template_file\" . }}/pod_template_file.yaml'\n    worker_container_repository: '{{ .Values.images.airflow.repository | default .Values.defaultAirflowRepository }}'\n    worker_container_tag: '{{ .Values.images.airflow.tag | default .Values.defaultAirflowTag }}'\n    multi_namespace_mode: '{{ if .Values.multiNamespaceMode }}True{{ else }}False{{ end }}'\n# yamllint enable rule:line-length\n\n# Whether Airflow can launch workers and/or pods in multiple namespaces\n# If true, it creates ClusterRole/ClusterRolebinding (with access to entire cluster)\nmultiNamespaceMode: false\n\n# `podTemplate` is a templated string containing the contents of `pod_template_file.yaml` used for\n# KubernetesExecutor workers. The default `podTemplate` will use normal `workers` configuration parameters\n# (e.g. `workers.resources`). As such, you normally won't need to override this directly, however,\n# you can still provide a completely custom `pod_template_file.yaml` if desired.\n# If not set, a default one is created using `files/pod-template-file.kubernetes-helm-yaml`.\npodTemplate: ~\n# The following example is NOT functional, but meant to be illustrative of how you can provide a custom\n# `pod_template_file`. You're better off starting with the default in\n# `files/pod-template-file.kubernetes-helm-yaml` and modifying from there.\n# We will set `priorityClassName` in this example:\n# podTemplate: |\n#   apiVersion: v1\n#   kind: Pod\n#   metadata:\n#     name: dummy-name\n#     labels:\n#       tier: airflow\n#       component: worker\n#       release: {{ .Release.Name }}\n#   spec:\n#     priorityClassName: high-priority\n#     containers:\n#       - name: base\n#         ...\n\n# Git sync\ndags:\n  persistence:\n    # Enable persistent volume for storing dags\n    enabled: false\n    # Volume size for dags\n    size: 1Gi\n    # If using a custom storageClass, pass name here\n    storageClassName:\n    # access mode of the persistent volume\n    accessMode: ReadWriteOnce\n    ## the name of an existing PVC to use\n    existingClaim:\n  gitSync:\n    enabled: true\n\n    # git repo clone url\n    # ssh examples ssh://git@github.com/apache/airflow.git\n    # git@github.com:apache/airflow.git\n    # https example: https://github.com/apache/airflow.git\n    repo: https://github.com/MatheusKraisfeld/aws-censo-ed-superior.git\n    branch: dev\n    rev: HEAD\n    depth: 1\n    # the number of consecutive failures allowed before aborting\n    maxFailures: 0\n    # subpath within the repo where dags are located\n    # should be \"\" if dags are at repo root\n    subPath: \"airflow/dags\"\n    # if your repo needs a user name password\n    # you can load them to a k8s secret like the one below\n    #   ---\n    #   apiVersion: v1\n    #   kind: Secret\n    #   metadata:\n    #     name: git-credentials\n    #   data:\n    #     GIT_SYNC_USERNAME: \u003cbase64_encoded_git_username\u003e\n    #     GIT_SYNC_PASSWORD: \u003cbase64_encoded_git_password\u003e\n    # and specify the name of the secret below\n    #\n    # credentialsSecret: git-credentials\n    #\n    #\n    # If you are using an ssh clone url, you can load\n    # the ssh private key to a k8s secret like the one below\n    #   ---\n    #   apiVersion: v1\n    #   kind: Secret\n    #   metadata:\n    #     name: airflow-ssh-secret\n    #   data:\n    #     # key needs to be gitSshKey\n    #     gitSshKey: \u003cbase64_encoded_data\u003e\n    # and specify the name of the secret below\n    # sshKeySecret: airflow-ssh-secret\n    #\n    # If you are using an ssh private key, you can additionally\n    # specify the content of your known_hosts file, example:\n    #\n    # knownHosts: |\n    #    \u003chost1\u003e,\u003cip1\u003e \u003ckey1\u003e\n    #    \u003chost2\u003e,\u003cip2\u003e \u003ckey2\u003e\n    # interval between git sync attempts in seconds\n    wait: 60\n    containerName: git-sync\n    uid: 65533\n\n    # When not set, the values defined in the global securityContext will be used\n    securityContext: {}\n    #  runAsUser: 65533\n    #  runAsGroup: 0\n\n    extraVolumeMounts: []\n    env: []\n    resources: {}\n    #  limits:\n    #   cpu: 100m\n    #   memory: 128Mi\n    #  requests:\n    #   cpu: 100m\n    #   memory: 128Mi\n\nlogs:\n  persistence:\n    # Enable persistent volume for storing logs\n    enabled: false\n    # Volume size for logs\n    size: 100Gi\n    # If using a custom storageClass, pass name here\n    storageClassName:\n    ## the name of an existing PVC to use\n    existingClaim:\n\n---\n# Role for spark-on-k8s-operator to create resources on cluster\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: spark-cluster-cr\n  labels:\n    rbac.authorization.kubeflow.org/aggregate-to-kubeflow-edit: \"true\"\nrules:\n  - apiGroups:\n      - sparkoperator.k8s.io\n    resources:\n      - sparkapplications\n    verbs:\n      - '*'\n---\n# Allow airflow-worker service account access for spark-on-k8s\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: airflow-spark-crb\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: spark-cluster-cr\nsubjects:\n  - kind: ServiceAccount\n    name: airflow-worker\n    namespace: airflow\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: airflow\nspec:\n  type: LoadBalancer\n  selector:\n    app: airflow\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080"
            ],
            "verify": false,
            "version": "8.6.1",
            "wait": true
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "data.aws_eks_cluster.cluster"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "airflow",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "airflow",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "airflow",
                "resource_version": "3980",
                "self_link": "",
                "uid": "ed7df558-0774-4c99-ae4d-f312854e3c3d"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "data.aws_eks_cluster.cluster"
          ]
        }
      ]
    }
  ]
}
